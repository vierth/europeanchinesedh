{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ec165f6",
   "metadata": {},
   "source": [
    "# European Summer School in Chinese Digital Humanities\n",
    "\n",
    "## Word embeddings\n",
    "In this notebook I will introduce a script that will allow you to conduct stylometric analysis by only changing a few options. This notebook will perform heirarchical cluster analysis.\n",
    "\n",
    "### The imports\n",
    "There are a number of items from various Python librarys that we need to import to conduct the analysis we are interested in. It is, of course, possible for us to write all of the code necessary for this ourselves, but it is much preferable to rely on things that other people have created for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8beda25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library for loading data\n",
    "import os, re\n",
    "\n",
    "# Libraries for analysis\n",
    "import gensim\n",
    "import pandas as pd\n",
    "\n",
    "# Library for visualization\n",
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Custom local modules with useful utilities\n",
    "from clean import clean # for cleaning the text\n",
    "from totrad import Convert # to convert to tradtitional characters\n",
    "\n",
    "# set gdrive_loc (this is only necessary when using these notebooks via google_collab)\n",
    "gdrive_loc = \"My Drive/europeanchinesedh-main\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed56baf3",
   "metadata": {},
   "source": [
    "### Set analysis options\n",
    "\n",
    "#### corpus_folder_name\n",
    "Provide the name of the corpus folder in a string. Leave as \"demo_corpus\" to use the supplied corpus.\n",
    "\n",
    "#### convert_to_traditional\n",
    "Set this to False to not modify the characters in the files. Set it to True if you would like to perform auto conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9739638",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_folder_name = \"demo_corpus\"\n",
    "\n",
    "convert_to_traditional = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9a54ac",
   "metadata": {},
   "source": [
    "From this point on you don't need to change any of the code to run the analysis, but you are welcome to mix things up if you like.\n",
    "\n",
    "### Tokenization note\n",
    "Note that in this case we are using a custom tokenization routine because gensim expects a different type of setup than we saw while using sklearn. It expects a list of lists in which the outer list is all of the sentences in the corpus and the inner lists are each token in each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d65c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_tokenize(text, sent_div=r\"[。！？]\"):\n",
    "    return re.split(sent_div, text)\n",
    "    \n",
    "def tokenize(text, n=1):\n",
    "    return [[sentence[i:i+n] for i in range(0, len(sentence), n)] for sentence in sent_tokenize(text)]\n",
    "\n",
    "ignore = {\".DS_Store\", \".txt\"}\n",
    "\n",
    "sentences = []\n",
    "\n",
    "if gdrive_loc:\n",
    "    corpus_folder_name = f\"{gdrive_loc}/{corpus_folder_name}\"\n",
    "\n",
    "for root, dirs, files in os.walk(corpus_folder_name):\n",
    "    for filename in files:\n",
    "        if filename not in ignore:\n",
    "            with open(os.path.join(root,filename), 'r', encoding='utf8') as rf:\n",
    "                # gensim's word2vec model expects sentences that contain words \n",
    "                # so we need to do a bit of pre processing. let's write a \n",
    "                # tokenization function!\n",
    "                \n",
    "\n",
    "                # if covert_to_traditional is set to True, convert\n",
    "                if convert_to_traditional:\n",
    "                    text = c.to_trad(rf.read())\n",
    "                else:\n",
    "                    text = rf.read()\n",
    "            \n",
    "                sentences.extend(tokenize(text))\n",
    "\n",
    "\n",
    "# create the model and tune it!\n",
    "word2Vec_model = gensim.models.Word2Vec(\n",
    "                    sentences, # input sentences\n",
    "                    sg=1, # use skip grams (0=CBOW). skip grams tend to work better on smaller corpora\n",
    "                    vector_size=100, # this is how many dimensions the vectors will be\n",
    "                    min_count=5 # how many times must a word apper\n",
    "                    )\n",
    "\n",
    "# we can update the model with train\n",
    "# word2Vec_model.train([[\"four\", \"score\", \"and\", \"seven\", \"years\", \"ago\"]], \n",
    "#                     total_examples=word2Vec_model.corpus_count,\n",
    "#                     epochs=word2Vec_model.epochs)\n",
    "\n",
    "# Let's save the model to file!\n",
    "# word2Vec_model.save('my_vecs.p')\n",
    "\n",
    "# let's take a look at some of the methods you can use:\n",
    "# most similar\n",
    "print(word2Vec_model.wv.most_similar('之'))\n",
    "\n",
    "# calculate similarity (cosine similarity)\n",
    "# distance will calculate euclidean distance\n",
    "print(word2Vec_model.wv.similarity('之', '的')) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d3274a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensionaliity reduction for visualization\n",
    "my_pca = PCA(n_components=3)\n",
    "\n",
    "word_vecs = word2Vec_model.wv\n",
    "vocab = word_vecs.key_to_index.keys()\n",
    "vecs = [word_vecs[word] for word in vocab]\n",
    "    \n",
    "transformed_data = my_pca.fit_transform(vecs)\n",
    "df = pd.DataFrame(index=vocab, columns=['Dim1', 'Dim2', 'Dim3'], data=transformed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49586555",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(df, x='Dim1', y='Dim2', z='Dim3', text=vocab)\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

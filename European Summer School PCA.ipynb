{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ec165f6",
   "metadata": {},
   "source": [
    "# European Summer School in Chinese Digital Humanities\n",
    "\n",
    "## Stylometry: PCA\n",
    "In this notebook I will introduce a script that will allow you to conduct stylometric analysis by only changing a few options. This notebook will perform principal component analysis.\n",
    "\n",
    "### The imports\n",
    "There are a number of items from various Python librarys that we need to import to conduct the analysis we are interested in. It is, of course, possible for us to write all of the code necessary for this ourselves, but it is much preferable to rely on things that other people have created for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aad46ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "%cd drive/MyDrive/europeanchinesedh-main/\n",
    "\n",
    "# set up Chinese font\n",
    "!wget -O TaipeiSansTCBeta-Regular.ttf https://drive.google.com/uc?id=1eGAsTN1HBpJAkeVM57_C7ccp7hbgSz3_&export=download\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib.font_manager import fontManager\n",
    "\n",
    "fontManager.addfont('TaipeiSansTCBeta-Regular.ttf')\n",
    "mpl.rc('font', family='Taipei Sans TC Beta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8beda25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library for loading and exporting data\n",
    "import os, json\n",
    "\n",
    "# Libraries for analysis\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Library for visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.colors\n",
    "\n",
    "# Custom local modules with useful utilities\n",
    "from clean import clean # for cleaning the text\n",
    "from totrad import Convert # to convert to tradtitional characers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed56baf3",
   "metadata": {},
   "source": [
    "### Set analysis options\n",
    "\n",
    "#### corpus_folder_name\n",
    "Provide the name of the corpus folder in a string. Leave as \"demo_corpus\" to use the supplied corpus.\n",
    "\n",
    "#### analysis_vocab_file\n",
    "If you want to provide a custom set of words to use for the analysis provide the name of a text file that contains the words, one word to a line. This should be a string like\n",
    "\"analysis_vocab.txt\"\n",
    "\n",
    "#### most_common_words\n",
    "Set the number of most common terms to use for your analysis. This is ignored if you provide a vocab file. By default this is set to None, which will analyze every word in the corpus. This should be an integer like\n",
    "100\n",
    "\n",
    "#### n_gram\n",
    "By default this is set to work on <i>n</i>-grams where n is 1, meaning individual characters will be at the root of the analysis. You are welcome to play with around with this as you see fit. The higher the n, the sparser the data.\n",
    "\n",
    "\n",
    "#### convert_to_traditional\n",
    "Set this to False to not modify the characters in the files. Set it to True if you would like to perform autoconversion\n",
    "\n",
    "### pca_components\n",
    "Set this integer to the number of components you would like the algorithm to return. This notebook is only set up to visualize the first 2 components, but this is available for your use should you like to dive further in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9739638",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_folder_name = \"demo_corpus\"\n",
    "\n",
    "analysis_vocab_file = None\n",
    "\n",
    "most_common_words = 100\n",
    "\n",
    "n_gram = 1\n",
    "\n",
    "convert_to_traditional = False\n",
    "\n",
    "pca_components = 2 # Only useful for digging even deeper in the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af184c9",
   "metadata": {},
   "source": [
    "## Adjustable parameters: Appearance\n",
    "These parameters will help you set the appearance of the plot itself.\n",
    "\n",
    "### label_types \n",
    "A tuple that specifies the nature of the corpus labeling. Here, the sample corpus files are all named with the convention author_title_section_genre.txt. Each type of label is one element in this tuple, in the same order they appear in the name\n",
    "\n",
    "### color_value \n",
    "this integer specifies which label should be used to generate a color scheme for the plot. 2 points to the 3rd element in the tuple, the siku categorization. There are three different siku categories reflected in the dataset, making this a good option. Here you should pick whichever label your analysis is focused on. More than 8 or so elements, however, will generate colors that are hard to tell apart.\n",
    "\n",
    "### label_value \n",
    "this integer specifies which label should be used for labeling the points in the plot. 0 points to the 1st element in the tuple, the title.\n",
    "\n",
    "### point_size \n",
    "is an integer that sets how large the points in the plot tare\n",
    "\n",
    "### point_labels \n",
    "is a boolean (True or False) that specifies if the points should be labeled.\n",
    "\n",
    "### plot_loadings \n",
    "is a boolean that specifies if the vocabulary should be drawn on the plot (which will aid in interpretation). The further a term is from the center of the plot, the more it is influencing texts in a given direction.\n",
    "\n",
    "### hide_points \n",
    "is a boolean that specifies of the points should be drawn. Set to False to see the loadings better.\n",
    "\n",
    "### output_dimensions \n",
    "is a tuple that sets the width and height of the output plot in inches. The inner values can be either integers or floats.\n",
    "\n",
    "### output_file \n",
    "contains the name of the file where the plot will be saved. The file extension will determine file type. png, pdf, jpg, tif, and others are all valid selections. On Macs, because of an oddity of the plotting library, pdfs will be very large. You can fix this by opening the file with adobe illustrator (or another similar program) and then saving a copy. This is because the entire font is embedded in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c26d65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Types of labels for documents in the corpus\n",
    "# This must match your metadata naming scheme!\n",
    "label_types = ('title', 'dynasty', 'siku', 'subcat', 'author') # tuple with strings\n",
    "\n",
    "# Some of these labels will set the color used to differentiate the points in the plot.\n",
    "# The label at this index is used to set Color:\n",
    "color_value = 3 # Index of label to use for color (integer). Here 3 points to \"genre\"\n",
    "\n",
    "# Index of label to use for plot labels (if points are labeled)\n",
    "label_value = 0 # Index of label to use for labels (integer). Here 0 points to \"title\"\n",
    "\n",
    "# Point size (integer)\n",
    "point_size = 8\n",
    "\n",
    "# Show point labels (add labels for each text):\n",
    "point_labels = False # True or False\n",
    "\n",
    "# Plot loadings (write the characters tot he plot)\n",
    "plot_loadings = False # True or False\n",
    "\n",
    "# Hide points (useful for seeing loadings better):\n",
    "hide_points = False # True or False\n",
    "\n",
    "# Output file info (dimensions are in inches (width, height)):\n",
    "output_dimensions = (10, 7.5) # Tuple of integers or floats\n",
    "\n",
    "# Output file extension determines output type. Save as a pdf if you want to edit in illustator\n",
    "# PDF Output on mac is very large, but just opening and saving a copy in illustrator will fix this\n",
    "output_file = \"myfigure.png\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af1ebed",
   "metadata": {},
   "source": [
    "From this point on you don't need to change any of the code to run the analysis, but you are welcome to mix things up if you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f89df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create containers for the data\n",
    "\n",
    "if convert_to_traditional:\n",
    "    c = Convert(preserve_multiple=False)\n",
    "\n",
    "if analysis_vocab_file:\n",
    "    with open(analysis_vocab_file, 'r', encoding='utf8') as rf:\n",
    "        vocab = [v for v in rf.read().split(\"\\n\") if v != \"\"]\n",
    "else:\n",
    "    vocab = None\n",
    "\n",
    "\n",
    "    \n",
    "##############\n",
    "# Load Texts #\n",
    "##############\n",
    "\n",
    "print(\"Loading, cleaning, and tokenizing\")\n",
    "# Go through each document in the corpus folder and save info to lists\n",
    "texts = []\n",
    "labels = []\n",
    "\n",
    "for root, dirs, files in os.walk(corpus_folder_name):\n",
    "    for i, f in enumerate(files):\n",
    "        if f.endswith(\".txt\"):\n",
    "            # add the labels to the label list\n",
    "            labels.append(f[:-4].split(\"_\"))\n",
    "\n",
    "            # Open the text, clean it, and tokenize it\n",
    "            with open(os.path.join(root,f),\"r\", encoding='utf8', errors='ignore') as rf:\n",
    "               # read and clean the file and append it to the texts list\n",
    "                text = clean(rf.read())\n",
    "\n",
    "                # if covert_to_traditional is set to True, convert\n",
    "                if convert_to_traditional:\n",
    "                    text = c.to_trad(text)\n",
    "                texts.append(text)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b977073",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# Perform Analysis #\n",
    "####################\n",
    "\n",
    "vectorizer = TfidfVectorizer(vocabulary=vocab, ngram_range=(n_gram, n_gram), max_features=most_common_words, use_idf=False, analyzer=\"char\")\n",
    "vecs = vectorizer.fit_transform(texts)\n",
    "vecs = vecs.toarray()\n",
    "\n",
    "# Lets perform PCA on the vectors:\n",
    "pca = PCA(n_components=pca_components)\n",
    "my_pca = pca.fit_transform(vecs)\n",
    "\n",
    "\n",
    "##############\n",
    "# Plot Setup #\n",
    "##############\n",
    "\n",
    "print(\"Setting plot info\")\n",
    "# set the plot size\n",
    "plt.figure(figsize=output_dimensions)\n",
    "\n",
    "# find all the unique values for each of the label types\n",
    "unique_label_values = [set() for i in range(len(label_types))]\n",
    "\n",
    "for label_list in labels:\n",
    "    for i, label in enumerate(label_list):\n",
    "        unique_label_values[i].add(label)\n",
    "\n",
    "# create color dictionaries for all labels\n",
    "color_dictionaries = []\n",
    "for unique_labels in unique_label_values:\n",
    "    colorpalette = sns.color_palette(\"husl\",len(unique_labels)).as_hex()\n",
    "    color_dictionaries.append(dict(zip(unique_labels,colorpalette)))\n",
    "\n",
    "# Now we need the Unique Labels\n",
    "unique_color_labels = list(unique_label_values[color_value])\n",
    "# Let's get a number for each class\n",
    "number_for_class = [i for i in range(len(unique_color_labels))]\n",
    "\n",
    "# Make a dictionary! This is new sytax for us! It just makes a dictionary where\n",
    "# the keys are the unique years and the values are found in number_for_class\n",
    "label_for_class_number = dict(zip(unique_color_labels,number_for_class))\n",
    "\n",
    "# Let's make a new representation for each document that is just these integers\n",
    "# and it needs to be a numpy array\n",
    "text_class = np.array([label_for_class_number[lab[color_value]] for lab in labels])\n",
    "\n",
    "\n",
    "# Make a list of the colors\n",
    "colors = [color_dictionaries[color_value][lab] for lab in unique_color_labels]\n",
    "\n",
    "if hide_points:\n",
    "    point_size = 0\n",
    "\n",
    "###################\n",
    "# Create the plot #\n",
    "###################\n",
    "\n",
    "print(\"Plotting texts\")\n",
    "for col, class_number, lab in zip(colors, number_for_class, unique_color_labels):\n",
    "    plt.scatter(my_pca[text_class==class_number,0],my_pca[text_class==class_number,1],label=lab,c=col, s=point_size)\n",
    "\n",
    "# Let's label individual points so we know WHICH document they are\n",
    "if point_labels:\n",
    "    print(\"Adding Labels\")\n",
    "    for lab, datapoint in zip(labels, my_pca):\n",
    "        plt.annotate(str(lab[label_value]),xy=datapoint)\n",
    "\n",
    "# Let's graph component loadings\n",
    "vocabulary = vectorizer.get_feature_names()\n",
    "loadings = pca.components_\n",
    "if plot_loadings:\n",
    "    print(\"Rendering Loadings\")    \n",
    "    for i, word in enumerate(vocabulary):\n",
    "        plt.annotate(word, xy=(loadings[0, i], loadings[1,i]))\n",
    "    \n",
    "\n",
    "# Let's add a legend! matplotlib will make this for us based on the data we \n",
    "# gave the scatter function.\n",
    "plt.legend()\n",
    "plt.savefig(output_file)\n",
    "\n",
    "\n",
    "############################################\n",
    "# Output data for JavaScript Visualization #\n",
    "############################################\n",
    "\n",
    "data = []\n",
    "for datapoint in my_pca:\n",
    "    pcDict = {}\n",
    "    for i, dp in enumerate(datapoint):\n",
    "        pcDict[f\"PC{str(i + 1)}\"] = dp\n",
    "    data.append(pcDict)\n",
    "\n",
    "jsLoadings = []\n",
    "for i, word in enumerate(vocabulary):\n",
    "    temploading = {}\n",
    "    for j,dp in enumerate(loadings):\n",
    "        temploading[f\"PC{str(j+1)}\"] = dp[i]\n",
    "    jsLoadings.append([word, temploading])\n",
    "\n",
    "color_dictionaries_list = []\n",
    "for cd in color_dictionaries:\n",
    "    cdlist = [v for v in cd.values()]\n",
    "    color_dictionaries_list.append(cdlist)\n",
    "\n",
    "colorstrings = json.dumps(color_dictionaries_list)\n",
    "labelstrings = json.dumps(labels)\n",
    "valuetypes = json.dumps([k for k in data[0].keys()])\n",
    "datastrings = json.dumps(data)\n",
    "\n",
    "limited_label_types = []\n",
    "for i, t in enumerate(label_types):\n",
    "    if len(unique_label_values[i]) <= 20:\n",
    "        limited_label_types.append(t)\n",
    "\n",
    "cattypestrings = json.dumps(limited_label_types)\n",
    "loadingstrings = json.dumps(jsLoadings)\n",
    "stringlist = [f\"var colorDictionaries = {colorstrings};\", f\"var labels = {labelstrings};\",\n",
    "            f\"var data = {datastrings};\", f\"var categoryTypes = {list(label_types)};\", \n",
    "            f\"var loadings = {jsLoadings};\", f\"var valueTypes = {valuetypes};\",\n",
    "            f\"var limitedCategories = {limited_label_types};\",\n",
    "            f\"var activecatnum = {color_value};\", f\"var activelabelnum = {label_value};\",\n",
    "            f\"var explainedvariance = [{round(pca.explained_variance_[0],3)},{round(pca.explained_variance_[1],3)}]\"]\n",
    "\n",
    "\n",
    "with open(\"pca_viz/data.js\", \"w\", encoding=\"utf8\") as wf:\n",
    "    wf.write(\"\\n\".join(stringlist))\n",
    "\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
